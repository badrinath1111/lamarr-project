{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import request",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from bs4 import beautifulsoup",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nimport json\nimport markdown\n\n# Define the URL of the website you want to scrape\nbase_url = \"https://prsindia.org/billtrack\"\npages_to_scrape = 5\noutput_file = \"scraped_data.json\"\n\ndef scrape_page(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.content, 'html.parser')\n        title = soup.title.text\n        content = soup.find('div', {'class': 'content'}).get_text()  # Update with your page structure\n        # Convert HTML tables to Markdown\n        for table in soup.find_all('table'):\n            markdown_table = markdown.markdown(str(table))\n            table.replace_with(BeautifulSoup(markdown_table, 'html.parser'))\n        return {'title': title, 'content': content}\n\n    return None\n\ndata = []\n\nfor page_num in range(1, pages_to_scrape + 1):\n    page_url = f\"{base_url}/page/{page_num}\"\n    page_data = scrape_page(page_url)\n    if page_data:\n        data.append(page_data)\n\n# Save the scraped data to a JSON file\nwith open(output_file, 'w') as json_file:\n    json.dump(data, json_file, indent=2)\n\nprint(f\"Scraped {len(data)} pages and saved to {output_file}\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}